#!/usr/bin/env python3
"""
Ollama CLI wrapper for Giant AI integration
"""

import sys
import os
import json
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from agent.providers.base import OllamaProvider

def main():
    # Read from stdin
    prompt = sys.stdin.read()
    
    # Create provider with config from environment
    config = {
        'ollama_base_url': os.environ.get('OLLAMA_BASE_URL', 'http://localhost:11434'),
        'ollama_model': os.environ.get('OLLAMA_MODEL', 'llama2'),
        'ollama_temperature': float(os.environ.get('OLLAMA_TEMPERATURE', '0.7')),
        'ollama_context_length': int(os.environ.get('OLLAMA_CONTEXT_LENGTH', '4096'))
    }
    
    try:
        provider = OllamaProvider(config)
        result = provider.execute_agent_task(prompt, {'project_context': ''})
        
        if result['success']:
            print(result['output'])
        else:
            print(f"Error: {result['error']}", file=sys.stderr)
            sys.exit(1)
            
    except Exception as e:
        print(f"Error: {str(e)}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()